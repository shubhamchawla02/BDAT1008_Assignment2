{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf26e2b",
   "metadata": {},
   "source": [
    "# BDAT 1008 - 04 Data Collection and Curation Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae550a4d",
   "metadata": {},
   "source": [
    "# Shubham Chawla 200493036"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f81ac",
   "metadata": {},
   "source": [
    "## Question 1 A sample schema for orders_data is given below. Import the data. How many unique cities are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5fc859d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ShubhamPC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d23c9d8610>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Necessary Libraries\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrameWriter as W\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark.conf.set('spark.sql.shuffle.partitions', 6)\n",
    "spark.conf.set('num-executors', 16)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9cf6fee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 3C52-E078\n",
      "\n",
      " Directory of C:\\Users\\shubh\\Desktop\\1008 Data Collection & Curation\\A2\\orders_data\n",
      "\n",
      "2022-07-10  05:59 PM    <DIR>          .\n",
      "2022-07-17  06:58 PM    <DIR>          ..\n",
      "2022-07-10  05:59 PM            14,173 orders_1.csv\n",
      "2022-07-10  05:59 PM            14,413 orders_10.csv\n",
      "2022-07-10  05:59 PM            14,392 orders_100.csv\n",
      "2022-07-10  05:59 PM            14,390 orders_11.csv\n",
      "2022-07-10  05:59 PM            14,432 orders_12.csv\n",
      "2022-07-10  05:59 PM            14,287 orders_13.csv\n",
      "2022-07-10  05:59 PM            14,354 orders_14.csv\n",
      "2022-07-10  05:59 PM            14,465 orders_15.csv\n",
      "2022-07-10  05:59 PM            14,442 orders_16.csv\n",
      "2022-07-10  05:59 PM            14,592 orders_17.csv\n",
      "2022-07-10  05:59 PM            14,439 orders_18.csv\n",
      "2022-07-10  05:59 PM            14,455 orders_19.csv\n",
      "2022-07-10  05:59 PM            14,485 orders_2.csv\n",
      "2022-07-10  05:59 PM            14,493 orders_20.csv\n",
      "2022-07-10  05:59 PM            14,461 orders_21.csv\n",
      "2022-07-10  05:59 PM            14,449 orders_22.csv\n",
      "2022-07-10  05:59 PM            14,518 orders_23.csv\n",
      "2022-07-10  05:59 PM            14,416 orders_24.csv\n",
      "2022-07-10  05:59 PM            14,464 orders_25.csv\n",
      "2022-07-10  05:59 PM            14,492 orders_26.csv\n",
      "2022-07-10  05:59 PM            14,432 orders_27.csv\n",
      "2022-07-10  05:59 PM            14,391 orders_28.csv\n",
      "2022-07-10  05:59 PM            14,414 orders_29.csv\n",
      "2022-07-10  05:59 PM            14,460 orders_3.csv\n",
      "2022-07-10  05:59 PM            14,489 orders_30.csv\n",
      "2022-07-10  05:59 PM            14,488 orders_31.csv\n",
      "2022-07-10  05:59 PM            14,419 orders_32.csv\n",
      "2022-07-10  05:59 PM            14,451 orders_33.csv\n",
      "2022-07-10  05:59 PM            14,406 orders_34.csv\n",
      "2022-07-10  05:59 PM            14,378 orders_35.csv\n",
      "2022-07-10  05:59 PM            14,449 orders_36.csv\n",
      "2022-07-10  05:59 PM            14,406 orders_37.csv\n",
      "2022-07-10  05:59 PM            14,470 orders_38.csv\n",
      "2022-07-10  05:59 PM            14,460 orders_39.csv\n",
      "2022-07-10  05:59 PM            14,156 orders_4.csv\n",
      "2022-07-10  05:59 PM            14,565 orders_40.csv\n",
      "2022-07-10  05:59 PM            14,655 orders_41.csv\n",
      "2022-07-10  05:59 PM            14,438 orders_42.csv\n",
      "2022-07-10  05:59 PM            14,350 orders_43.csv\n",
      "2022-07-10  05:59 PM            14,477 orders_44.csv\n",
      "2022-07-10  05:59 PM            14,445 orders_45.csv\n",
      "2022-07-10  05:59 PM            14,414 orders_46.csv\n",
      "2022-07-10  05:59 PM            14,506 orders_47.csv\n",
      "2022-07-10  05:59 PM            14,524 orders_48.csv\n",
      "2022-07-10  05:59 PM            14,486 orders_49.csv\n",
      "2022-07-10  05:59 PM            14,313 orders_5.csv\n",
      "2022-07-10  05:59 PM            14,427 orders_50.csv\n",
      "2022-07-10  05:59 PM            14,502 orders_51.csv\n",
      "2022-07-10  05:59 PM            14,419 orders_52.csv\n",
      "2022-07-10  05:59 PM            14,477 orders_53.csv\n",
      "2022-07-10  05:59 PM            14,427 orders_54.csv\n",
      "2022-07-10  05:59 PM            14,470 orders_55.csv\n",
      "2022-07-10  05:59 PM            14,420 orders_56.csv\n",
      "2022-07-10  05:59 PM            14,534 orders_57.csv\n",
      "2022-07-10  05:59 PM            14,442 orders_58.csv\n",
      "2022-07-10  05:59 PM            14,482 orders_59.csv\n",
      "2022-07-10  05:59 PM            14,308 orders_6.csv\n",
      "2022-07-10  05:59 PM            14,482 orders_60.csv\n",
      "2022-07-10  05:59 PM            14,495 orders_61.csv\n",
      "2022-07-10  05:59 PM            14,476 orders_62.csv\n",
      "2022-07-10  05:59 PM            14,441 orders_63.csv\n",
      "2022-07-10  05:59 PM            14,471 orders_64.csv\n",
      "2022-07-10  05:59 PM            14,450 orders_65.csv\n",
      "2022-07-10  05:59 PM            14,416 orders_66.csv\n",
      "2022-07-10  05:59 PM            14,384 orders_67.csv\n",
      "2022-07-10  05:59 PM            14,545 orders_68.csv\n",
      "2022-07-10  05:59 PM            14,407 orders_69.csv\n",
      "2022-07-10  05:59 PM            14,473 orders_7.csv\n",
      "2022-07-10  05:59 PM            14,423 orders_70.csv\n",
      "2022-07-10  05:59 PM            14,571 orders_71.csv\n",
      "2022-07-10  05:59 PM            14,433 orders_72.csv\n",
      "2022-07-10  05:59 PM            14,488 orders_73.csv\n",
      "2022-07-10  05:59 PM            14,480 orders_74.csv\n",
      "2022-07-10  05:59 PM            14,537 orders_75.csv\n",
      "2022-07-10  05:59 PM            14,465 orders_76.csv\n",
      "2022-07-10  05:59 PM            14,421 orders_77.csv\n",
      "2022-07-10  05:59 PM            14,332 orders_78.csv\n",
      "2022-07-10  05:59 PM            14,402 orders_79.csv\n",
      "2022-07-10  05:59 PM            14,465 orders_8.csv\n",
      "2022-07-10  05:59 PM            14,460 orders_80.csv\n",
      "2022-07-10  05:59 PM            14,457 orders_81.csv\n",
      "2022-07-10  05:59 PM            14,478 orders_82.csv\n",
      "2022-07-10  05:59 PM            14,419 orders_83.csv\n",
      "2022-07-10  05:59 PM            14,398 orders_84.csv\n",
      "2022-07-10  05:59 PM            14,429 orders_85.csv\n",
      "2022-07-10  05:59 PM            14,331 orders_86.csv\n",
      "2022-07-10  05:59 PM            14,510 orders_87.csv\n",
      "2022-07-10  05:59 PM            14,345 orders_88.csv\n",
      "2022-07-10  05:59 PM            14,364 orders_89.csv\n",
      "2022-07-10  05:59 PM            14,434 orders_9.csv\n",
      "2022-07-10  05:59 PM            14,393 orders_90.csv\n",
      "2022-07-10  05:59 PM            14,449 orders_91.csv\n",
      "2022-07-10  05:59 PM            14,496 orders_92.csv\n",
      "2022-07-10  05:59 PM            14,474 orders_93.csv\n",
      "2022-07-10  05:59 PM            14,465 orders_94.csv\n",
      "2022-07-10  05:59 PM            14,388 orders_95.csv\n",
      "2022-07-10  05:59 PM            14,369 orders_96.csv\n",
      "2022-07-10  05:59 PM            14,482 orders_97.csv\n",
      "2022-07-10  05:59 PM            14,480 orders_98.csv\n",
      "2022-07-10  05:59 PM            14,424 orders_99.csv\n",
      "             100 File(s)      1,444,064 bytes\n",
      "               2 Dir(s)  78,929,788,928 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls orders_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6fdb08fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.StructType'>\n"
     ]
    }
   ],
   "source": [
    "# Importing the data and applying sample schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "file_location = \"orders_data\\orders_*.csv\"\n",
    "\n",
    "ordersSchema = StructType([\n",
    "  StructField(\"Order_ID\", DoubleType(), True),\n",
    "  StructField(\"Country\", StringType(), True),\n",
    "  StructField(\"Province\", StringType(), True),\n",
    "  StructField(\"City\", StringType(), True),\n",
    "  StructField(\"Latitude\", DoubleType(), True),\n",
    "  StructField(\"Longitude\", DoubleType(), True),\n",
    "  StructField(\"TimeStamp\", StringType(), True),\n",
    "  StructField(\"Sales_Volume\", DoubleType(), True)])\n",
    "\n",
    "print(type(ordersSchema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "393cc547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 8)\n",
      "+--------+-------+--------+-------------+---------+---------+-------------------+------------+\n",
      "|Order_ID|Country|Province|         City| Latitude|Longitude|          TimeStamp|Sales_Volume|\n",
      "+--------+-------+--------+-------------+---------+---------+-------------------+------------+\n",
      "|231542.0| Canada|      AB|      Calgary|-113.9835|-113.9389|2022/04/22 08:28:49|       41.49|\n",
      "|473450.0| Canada|      AB|     Edmonton|-113.4467|-113.3654|2022/04/22 05:04:24|       48.52|\n",
      "|376604.0| Canada|      AB| Medicine Hat|-110.5798|-110.4884|2022/04/22 18:14:14|       60.79|\n",
      "|440105.0| Canada|      AB|Sherwood Park|-113.2427| -113.242|2022/04/22 11:27:20|       77.81|\n",
      "|483058.0| Canada|      AB|     Beaumont|-113.3783|-113.2894|2022/04/22 21:04:24|       12.06|\n",
      "+--------+-------+--------+-------------+---------+---------+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating DataFrame \n",
    "data = (\n",
    "  spark\n",
    "    .read\n",
    "    .schema(ordersSchema)\n",
    "    .csv(file_location)\n",
    ")\n",
    "\n",
    "# create table for SQL analytics\n",
    "data.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# Checking Size of pyspark dataframe\n",
    "print((data.count(), len(data.columns)))\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7fc52034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 648 duplicate OrderIDs\n"
     ]
    }
   ],
   "source": [
    "# Checking for duplicate order ids\n",
    "if data.count() > data.dropDuplicates(['Order_ID']).count():\n",
    "    print('Data has {} duplicate OrderIDs'.format(data.count() - data.dropDuplicates(['Order_ID']).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b88597be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19352, 8)\n"
     ]
    }
   ],
   "source": [
    "# Removing Duplicate OrderIDs\n",
    "duplicates_removed_df = data.dropDuplicates(['Order_ID'])\n",
    "# create table for SQL analytics - duplicates removed\n",
    "duplicates_removed_df.createOrReplaceTempView(\"orders_duplicates_removed\")\n",
    "# Checking Size of dataframe after removing duplicates\n",
    "print((duplicates_removed_df.count(), len(duplicates_removed_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "250a5ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Unique Cities Count|\n",
      "+-------------------+\n",
      "|                619|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unique Cities Count without removing duplicates\n",
    "df_sql = spark.sql(\"SELECT count(distinct(City)) AS `Unique Cities Count` FROM orders\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "280b9ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Unique Cities Count|\n",
      "+-------------------+\n",
      "|                612|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unique Cities Count after removing duplicate Order_ID\n",
    "df_sql = spark.sql(\"SELECT count(distinct(City)) AS `Unique Cities Count` FROM orders_duplicates_removed\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3a9a9",
   "metadata": {},
   "source": [
    "## Question 2 For each province, show the latest time of the order that was made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5282d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Province|Latest Time of Order|\n",
      "+--------+--------------------+\n",
      "|      AB| 2022/04/23 01:13:04|\n",
      "|      BC| 2022/04/23 01:05:14|\n",
      "|      MB| 2022/04/23 00:25:42|\n",
      "|      NB| 2022/04/23 00:47:52|\n",
      "|      NL| 2022/04/23 00:36:20|\n",
      "|      NS| 2022/04/23 00:19:17|\n",
      "|      NT| 2022/04/22 17:33:18|\n",
      "|      ON| 2022/04/23 01:18:38|\n",
      "|      PE| 2022/04/22 23:23:21|\n",
      "|      QC| 2022/04/23 01:17:12|\n",
      "|      SK| 2022/04/23 01:19:30|\n",
      "|      YT| 2022/04/22 20:48:47|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "select Province, max(TimeStamp) AS `Latest Time of Order`\n",
    "from orders_duplicates_removed\n",
    "group by Province\n",
    "order by Province\n",
    "\"\"\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5162c6",
   "metadata": {},
   "source": [
    "## Question 3 A point of interest (POI) is a specific point location that people take interest. Import the POI data set (POI.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e9cc863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data using inferred schema\n",
    "# Location and type of the file being imported\n",
    "path = 'POI.csv'\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV OPTIONS\n",
    "infer_schema = \"true\" #By using this option spark will automatically go through the csv file and infer the schema of each column.\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Importing POI.csv file\n",
    "df_POI = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\",infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "64ebf6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POI_ID', ' Latitude', 'Longitude']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_POI.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1cc13dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POI_ID', 'Latitude', 'Longitude']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming column Latitude to remove an extra space in the beginning\n",
    "df_POI = df_POI.withColumnRenamed(\" Latitude\",\"Latitude\")\n",
    "df_POI.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9bcefb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----------+\n",
      "|POI_ID| Latitude|  Longitude|\n",
      "+------+---------+-----------+\n",
      "| POI-1|45.521629| -74.085634|\n",
      "| POI-2|53.596345|-114.465122|\n",
      "| POI-3|44.897823| -62.987528|\n",
      "+------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create view for SQL analytics\n",
    "df_POI.createOrReplaceTempView(\"POI\")\n",
    "df_POI.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583611f7",
   "metadata": {},
   "source": [
    "## Question 4 For each Order_ID and POI pair, get the distance between the Order_ID and the POI based on the geographic Latitude and Longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "37ef1431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+---------+---------+-----------+\n",
      "|Order_ID|POI_ID|    Lat_A|   Long_A|    Lat_B|     Long_B|\n",
      "+--------+------+---------+---------+---------+-----------+\n",
      "|  250410| POI-1|-113.2142|-113.1836|45.521629| -74.085634|\n",
      "|  250410| POI-2|-113.2142|-113.1836|53.596345|-114.465122|\n",
      "|  250410| POI-3|-113.2142|-113.1836|44.897823| -62.987528|\n",
      "|  286817| POI-1|-111.4565|-111.4168|45.521629| -74.085634|\n",
      "|  286817| POI-2|-111.4565|-111.4168|53.596345|-114.465122|\n",
      "|  286817| POI-3|-111.4565|-111.4168|44.897823| -62.987528|\n",
      "|  298450| POI-1|-113.3683|-113.3579|45.521629| -74.085634|\n",
      "|  298450| POI-2|-113.3683|-113.3579|53.596345|-114.465122|\n",
      "|  298450| POI-3|-113.3683|-113.3579|44.897823| -62.987528|\n",
      "|  480255| POI-1| -79.3772| -79.3679|45.521629| -74.085634|\n",
      "+--------+------+---------+---------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining views orders and POI (A = orders, B = POI)\n",
    "combined_df = spark.sql(\n",
    "    \"\"\"select int(v1.Order_ID), v2.POI_ID, v1.Latitude AS `Lat_A`, v1.Longitude AS `Long_A`, v2.Latitude AS `Lat_B`, v2.Longitude AS `Long_B`\n",
    "    from orders_duplicates_removed v1, POI v2\n",
    "    \"\"\"\n",
    ")\n",
    "combined_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5879a714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"formula.png\" width=\"600\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"formula.png\", width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a2b21400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+---------+---------+-----------+---------+\n",
      "|Order_ID|POI_ID|    Lat_A|   Long_A|    Lat_B|     Long_B| Distance|\n",
      "+--------+------+---------+---------+---------+-----------+---------+\n",
      "|  250410| POI-1|-113.2142|-113.1836|45.521629| -74.085634|16731.229|\n",
      "|  250410| POI-2|-113.2142|-113.1836|53.596345|-114.465122| 18546.88|\n",
      "|  250410| POI-3|-113.2142|-113.1836|44.897823| -62.987528| 16216.34|\n",
      "|  286817| POI-1|-111.4565|-111.4168|45.521629| -74.085634|16702.623|\n",
      "|  286817| POI-2|-111.4565|-111.4168|53.596345|-114.465122|18345.494|\n",
      "|  286817| POI-3|-111.4565|-111.4168|44.897823| -62.987528|16232.481|\n",
      "|  298450| POI-1|-113.3683|-113.3579|45.521629| -74.085634|16731.879|\n",
      "|  298450| POI-2|-113.3683|-113.3579|53.596345|-114.465122|18564.407|\n",
      "|  298450| POI-3|-113.3683|-113.3579|44.897823| -62.987528|16213.075|\n",
      "|  480255| POI-1| -79.3772| -79.3679|45.521629| -74.085634|13892.397|\n",
      "|  480255| POI-2| -79.3772| -79.3679|53.596345|-114.465122|14961.485|\n",
      "|  480255| POI-3| -79.3772| -79.3679|44.897823| -62.987528|13859.726|\n",
      "|  433267| POI-1|  -82.978|  -82.895|45.521629| -74.085634|14296.757|\n",
      "|  433267| POI-2|  -82.978|  -82.895|53.596345|-114.465122|15286.753|\n",
      "|  433267| POI-3|  -82.978|  -82.895|44.897823| -62.987528|14261.037|\n",
      "|  200925| POI-1| -79.3595| -79.3471|45.521629| -74.085634|13890.402|\n",
      "|  200925| POI-2| -79.3595| -79.3471|53.596345|-114.465122|14959.962|\n",
      "|  200925| POI-3| -79.3595| -79.3471|44.897823| -62.987528|13857.713|\n",
      "|  346582| POI-1|-114.3696|-114.3627|45.521629| -74.085634|16736.781|\n",
      "|  346582| POI-2|-114.3696|-114.3627|53.596345|-114.465122|18676.975|\n",
      "+--------+------+---------+---------+---------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating distance between two points using Haversine Distance formula\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "combined_df = combined_df.withColumn(\"a\", (\n",
    "        F.pow(F.sin(F.radians(F.col(\"Lat_B\") - F.col(\"Lat_A\")) / 2), 2) +\n",
    "        F.cos(F.radians(F.col(\"Lat_A\"))) * F.cos(F.radians(F.col(\"Lat_B\"))) *\n",
    "        F.pow(F.sin(F.radians(F.col(\"Long_B\") - F.col(\"Long_A\")) / 2), 2)\n",
    "    )).withColumn(\"Distance\", F.atan2(F.sqrt(F.col(\"a\")), F.sqrt(-F.col(\"a\") + 1)) * 12742.018)  #2*uE = 2 * 6371.009\n",
    "\n",
    "combined_df = combined_df.drop('a')\n",
    "combined_df = combined_df.withColumn(\"Distance\", F.round(combined_df[\"Distance\"], 3))\n",
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a8208",
   "metadata": {},
   "source": [
    "## Question 5 For each Order_ID, identify the POI with the shortest distance. Retain only 1 record for each Order_ID. (Check: Your end result should have the same record count as your orders_data dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9732a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------------+\n",
      "|Order_ID|POI_ID|Shortest Distance|\n",
      "+--------+------+-----------------+\n",
      "|  200021| POI-3|        16204.454|\n",
      "|  200026| POI-3|        13901.091|\n",
      "|  200041| POI-3|         16209.39|\n",
      "|  200099| POI-3|        13889.543|\n",
      "|  200137| POI-3|        13954.091|\n",
      "|  200154| POI-3|        13873.513|\n",
      "|  200171| POI-3|        16167.789|\n",
      "|  200183| POI-3|        13188.207|\n",
      "|  200220| POI-3|        13856.457|\n",
      "|  200221| POI-3|        13880.355|\n",
      "|  200231| POI-3|        13857.506|\n",
      "|  200242| POI-3|        13877.028|\n",
      "|  200308| POI-3|        16213.982|\n",
      "|  200311| POI-3|        13840.551|\n",
      "|  200313| POI-3|        13857.452|\n",
      "|  200335| POI-3|        13621.158|\n",
      "|  200343| POI-3|        13883.572|\n",
      "|  200347| POI-3|        13194.087|\n",
      "|  200374| POI-3|        12933.142|\n",
      "|  200398| POI-3|        14923.097|\n",
      "+--------+------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create view for SQL analytics\n",
    "combined_df.createOrReplaceTempView(\"orders_POI\")\n",
    "\n",
    "# Finding POI with shortest distance for each Order_ID by using window function\n",
    "shortest_df = spark.sql(\n",
    "    \"\"\"WITH cte AS (\n",
    "        SELECT Order_ID, POI_ID, Distance,\n",
    "            RANK() OVER ( PARTITION BY Order_ID\n",
    "            ORDER BY Distance ASC\n",
    "            ) AS r\n",
    "        FROM orders_POI\n",
    "       )\n",
    "       SELECT Order_ID, POI_ID, Distance AS `Shortest Distance`\n",
    "       FROM cte\n",
    "       WHERE r = 1 \n",
    "       ORDER BY Order_ID;\n",
    "    \"\"\"\n",
    ")\n",
    "shortest_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1e32e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19352, 3)\n"
     ]
    }
   ],
   "source": [
    "# Checking size of dataframe after retaining only single record for each unique Order_ID\n",
    "print((shortest_df.count(), len(shortest_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9e88dad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19352, 8)\n"
     ]
    }
   ],
   "source": [
    "# Checking order_data dataset size with duplicates removed\n",
    "print((duplicates_removed_df.count(), len(duplicates_removed_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11ec2a",
   "metadata": {},
   "source": [
    "## Question 6 Based on #5, for each POI, get the average, standard deviation and max of the (shortest) distances, as well as the count of the orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3b99d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+---------------+--------+-----------+\n",
      "|POI_ID|Average_Distance|Stddev_Distance|   Max_D|Order_Count|\n",
      "+------+----------------+---------------+--------+-----------+\n",
      "| POI-3|       14872.704|       1310.943|16235.19|      19352|\n",
      "+------+----------------+---------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create view for SQL analytics\n",
    "shortest_df.createOrReplaceTempView(\"v6\")\n",
    "\n",
    "# Using Aggregate Functions\n",
    "descriptive_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT POI_ID, round(avg(`Shortest Distance`),3) AS Average_Distance, \n",
    "    round(stddev(`Shortest Distance`),3) AS Stddev_Distance, \n",
    "    max(`Shortest Distance`) AS `Max_D`, \n",
    "    count(Order_ID) AS `Order_Count`\n",
    "    FROM v6\n",
    "    Group by POI_ID\n",
    "    \"\"\"\n",
    ")\n",
    "descriptive_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65675354",
   "metadata": {},
   "source": [
    "## Question 7 \tFor each POI, based on the max distance and orders count from #6, calculate the density using the formula: density =orders_count / (π*(max_distance)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b39b3b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-----------------+\n",
      "|POI_ID|Order_Count|   Max_D|          Density|\n",
      "+------+-----------+--------+-----------------+\n",
      "| POI-3|      19352|16235.19|0.000023381989235|\n",
      "+------+-----------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create view for SQL analytics\n",
    "descriptive_df.createOrReplaceTempView(\"v7\")\n",
    "\n",
    "# Casting Density as Decimal Value\n",
    "density_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT POI_ID, Order_Count, Max_D, CAST(Order_Count/(3.14*pow(Max_D,2)) AS Decimal(25,15)) as Density\n",
    "    FROM v7\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "density_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
